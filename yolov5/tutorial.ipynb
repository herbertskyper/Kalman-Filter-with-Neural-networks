{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6MPjfT5NrKQ"
      },
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "  <a href=\"https://ultralytics.com/yolov5\" target=\"_blank\">\n",
        "    <img width=\"1024\", src=\"https://raw.githubusercontent.com/ultralytics/assets/main/yolov5/v70/splash.png\"></a>\n",
        "\n",
        "[‰∏≠Êñá](https://docs.ultralytics.com/zh/) | [ÌïúÍµ≠Ïñ¥](https://docs.ultralytics.com/ko/) | [Êó•Êú¨Ë™û](https://docs.ultralytics.com/ja/) | [–†—É—Å—Å–∫–∏–π](https://docs.ultralytics.com/ru/) | [Deutsch](https://docs.ultralytics.com/de/) | [Fran√ßais](https://docs.ultralytics.com/fr/) | [Espa√±ol](https://docs.ultralytics.com/es/) | [Portugu√™s](https://docs.ultralytics.com/pt/) | [‡§π‡§ø‡§®‡•ç‡§¶‡•Ä](https://docs.ultralytics.com/hi/) | [ÿßŸÑÿπÿ±ÿ®Ÿäÿ©](https://docs.ultralytics.com/ar/)\n",
        "\n",
        "  <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a>\n",
        "  <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n",
        "  <a href=\"https://www.kaggle.com/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n",
        "\n",
        "This <a href=\"https://github.com/ultralytics/yolov5\">YOLOv5</a> üöÄ notebook by <a href=\"https://ultralytics.com\">Ultralytics</a> presents simple train, validate and predict examples to help start your AI adventure.<br>We hope that the resources in this notebook will help you get the most out of YOLOv5. Please browse the YOLOv5 <a href=\"https://docs.ultralytics.com/yolov5\">Docs</a> for details, raise an issue on <a href=\"https://github.com/ultralytics/yolov5\">GitHub</a> for support, and join our <a href=\"https://ultralytics.com/discord\">Discord</a> community for questions and discussions!\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mGmQbAO5pQb"
      },
      "source": [
        "# Setup\n",
        "\n",
        "Clone GitHub [repository](https://github.com/ultralytics/yolov5), install [dependencies](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) and check PyTorch and GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbvMlHd_QwMG",
        "outputId": "e8225db4-e61d-4640-8b1f-8bfce3331cea"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "YOLOv5 üöÄ v7.0-306-gb599ae42 Python-3.9.7 torch-1.12.0 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 7933MiB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setup complete ‚úÖ (20 CPUs, 15.2 GB RAM, 237.2/871.8 GB disk)\n"
          ]
        }
      ],
      "source": [
        "# !git clone https://github.com/ultralytics/yolov5  # clone\n",
        "# %cd yolov5\n",
        "# %pip install -qr requirements.txt comet_ml  # install\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()  # checks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JnkELT0cIJg"
      },
      "source": [
        "# 1. Detect\n",
        "\n",
        "`detect.py` runs YOLOv5 inference on a variety of sources, downloading models automatically from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases), and saving results to `runs/detect`. Example inference sources are:\n",
        "\n",
        "```shell\n",
        "python detect.py --source 0  # webcam\n",
        "                          img.jpg  # image\n",
        "                          vid.mp4  # video\n",
        "                          screen  # screenshot\n",
        "                          path/  # directory\n",
        "                         'path/*.jpg'  # glob\n",
        "                         'https://youtu.be/LNwODJXcvt4'  # YouTube\n",
        "                         'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP stream\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zR9ZbuQCH7FX",
        "outputId": "284ef04b-1596-412f-88f6-948828dd2b49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['yolov5s.pt'], source=data/images, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_csv=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
            "YOLOv5 üöÄ v7.0-306-gb599ae42 Python-3.9.7 torch-1.12.0 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 7933MiB)\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
            "image 1/2 /home/herbert/Desktop/project/Kalman-Filter-with-Neural-networks/yolov5/data/images/bus.jpg: 640x480 4 persons, 1 bus, 17.7ms\n",
            "image 2/2 /home/herbert/Desktop/project/Kalman-Filter-with-Neural-networks/yolov5/data/images/zidane.jpg: 384x640 2 persons, 2 ties, 8.1ms\n",
            "Speed: 0.8ms pre-process, 12.9ms inference, 7.8ms NMS per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/exp9\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python detect.py --weights yolov5s.pt --img 640 --conf 0.25 --source data/images\n",
        "# display.Image(filename='runs/detect/exp/zidane.jpg', width=600)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkAzDWJ7cWTr"
      },
      "source": [
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
        "<img align=\"left\" src=\"https://user-images.githubusercontent.com/26833433/127574988-6a558aa1-d268-44b9-bf6b-62d4c605cc72.jpg\" width=\"600\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eq1SMWl6Sfn"
      },
      "source": [
        "# 2. Validate\n",
        "Validate a model's accuracy on the [COCO](https://cocodataset.org/#home) dataset's `val` or `test` splits. Models are downloaded automatically from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases). To show results by class use the `--verbose` flag."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQPtK1QYVaD_",
        "outputId": "cf7d52f0-281c-4c96-a488-79f5908f8426"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Download COCO val\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mhub\u001b[38;5;241m.\u001b[39mdownload_url_to_file(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://ultralytics.com/assets/coco2017val.zip\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtmp.zip\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# download (780M - 5000 images)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munzip -q tmp.zip -d ../datasets && rm tmp.zip  # unzip\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ],
      "source": [
        "# Download COCO val\n",
        "torch.hub.download_url_to_file('https://ultralytics.com/assets/coco2017val.zip', 'tmp.zip')  # download (780M - 5000 images)\n",
        "!unzip -q tmp.zip -d ../datasets && rm tmp.zip  # unzip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X58w8JLpMnjH",
        "outputId": "3e234e05-ee8b-4ad1-b1a4-f6a55d5e4f3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mdata=/home/herbert/Desktop/project/Kalman-Filter-with-Neural-networks/yolov5/data/coco.yaml, weights=['yolov5s.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.6, max_det=300, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=True, project=runs/val, name=exp, exist_ok=False, half=True, dnn=False\n",
            "YOLOv5 üöÄ v7.0-306-gb599ae42 Python-3.9.7 torch-1.12.0 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 7933MiB)\n",
            "\n",
            "Fusing layers... \n"
          ]
        }
      ],
      "source": [
        "# Validate YOLOv5s on COCO val\n",
        "!python val.py --weights yolov5s.pt --data coco.yaml --img 640 --half"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY2VXXXu74w5"
      },
      "source": [
        "# 3. Train\n",
        "\n",
        "<p align=\"\"><a href=\"https://bit.ly/ultralytics_hub\"><img width=\"1000\" src=\"https://github.com/ultralytics/assets/raw/main/im/integrations-loop.png\"/></a></p>\n",
        "Close the active learning loop by sampling images from your inference conditions with the `roboflow` pip package\n",
        "<br><br>\n",
        "\n",
        "Train a YOLOv5s model on the [COCO128](https://www.kaggle.com/ultralytics/coco128) dataset with `--data coco128.yaml`, starting from pretrained `--weights yolov5s.pt`, or from randomly initialized `--weights '' --cfg yolov5s.yaml`.\n",
        "\n",
        "- **Pretrained [Models](https://github.com/ultralytics/yolov5/tree/master/models)** are downloaded\n",
        "automatically from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases)\n",
        "- **[Datasets](https://github.com/ultralytics/yolov5/tree/master/data)** available for autodownload include: [COCO](https://github.com/ultralytics/yolov5/blob/master/data/coco.yaml), [COCO128](https://github.com/ultralytics/yolov5/blob/master/data/coco128.yaml), [VOC](https://github.com/ultralytics/yolov5/blob/master/data/VOC.yaml), [Argoverse](https://github.com/ultralytics/yolov5/blob/master/data/Argoverse.yaml), [VisDrone](https://github.com/ultralytics/yolov5/blob/master/data/VisDrone.yaml), [GlobalWheat](https://github.com/ultralytics/yolov5/blob/master/data/GlobalWheat2020.yaml), [xView](https://github.com/ultralytics/yolov5/blob/master/data/xView.yaml), [Objects365](https://github.com/ultralytics/yolov5/blob/master/data/Objects365.yaml), [SKU-110K](https://github.com/ultralytics/yolov5/blob/master/data/SKU-110K.yaml).\n",
        "- **Training Results** are saved to `runs/train/` with incrementing run directories, i.e. `runs/train/exp2`, `runs/train/exp3` etc.\n",
        "<br>\n",
        "\n",
        "A **Mosaic Dataloader** is used for training which combines 4 images into 1 mosaic.\n",
        "\n",
        "## Label a dataset on Roboflow (optional)\n",
        "\n",
        "[Roboflow](https://roboflow.com/?ref=ultralytics) enables you to easily **organize, label, and prepare** a high quality dataset with your own custom data. Roboflow also makes it easy to establish an active learning pipeline, collaborate with your team on dataset improvement, and integrate directly into your model building workflow with the `roboflow` pip package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "i3oKtE4g-aNn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Please paste your Comet API key from https://www.comet.com/api/my/settings/\n",
            "(api key may not show as you type)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Can not parse empty Comet API key\n",
            "\u001b[1;38;5;196mCOMET ERROR:\u001b[0m Unable to verify Comet API key at this time\n"
          ]
        }
      ],
      "source": [
        "#@title Select YOLOv5 üöÄ logger {run: 'auto'}\n",
        "logger = 'Comet' #@param ['Comet', 'ClearML', 'TensorBoard']\n",
        "\n",
        "if logger == 'Comet':\n",
        "  %pip install -q comet_ml\n",
        "  import comet_ml; comet_ml.init()\n",
        "elif logger == 'ClearML':\n",
        "  %pip install -q clearml\n",
        "  import clearml; clearml.browser_login()\n",
        "elif logger == 'TensorBoard':\n",
        "  %load_ext tensorboard\n",
        "  %tensorboard --logdir runs/train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NcFxRcFdJ_O",
        "outputId": "bbeeea2b-04fc-4185-aa64-258690495b5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=./models/yolov5s_qrcode.yaml, data=coco128.yaml, hyp=./data/hyps/hyp.scratch-low.yaml, epochs=3, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=0, project=runs/train_QR, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest, ndjson_console=False, ndjson_file=False\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mskipping check (offline), for updates see https://github.com/ultralytics/yolov5\n",
            "YOLOv5 üöÄ v7.0-306-gb599ae42 Python-3.9.7 torch-1.12.0 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 7933MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train_QR', view at http://localhost:6006/\n",
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Can not parse empty Comet API key\n",
            "COMET WARNING: Comet credentials have not been set. Comet will default to offline logging. Please set your credentials to enable online logging.\n",
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Can not parse empty Comet API key\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Using '/home/herbert/Desktop/project/Kalman-Filter-with-Neural-networks/yolov5/.cometml-runs' path as offline directory. Pass 'offline_directory' parameter into constructor or set the 'COMET_OFFLINE_DIRECTORY' environment variable to manually choose where to store offline experiment archives.\n",
            "\n",
            "Dataset not found ‚ö†Ô∏è, missing paths ['/home/herbert/Desktop/project/Kalman-Filter-with-Neural-networks/datasets/coco128/images/train2017']\n",
            "Downloading https://ultralytics.com/assets/coco128.zip to coco128.zip...\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6.66M/6.66M [00:00<00:00, 8.78MB/s]\n",
            "Dataset download success ‚úÖ (3.4s), saved to \u001b[1m/home/herbert/Desktop/project/Kalman-Filter-with-Neural-networks/datasets\u001b[0m\n",
            "Overriding model.yaml nc=1 with nc=80\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
            "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1    229245  models.yolo.Detect                      [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "YOLOv5s_qrcode summary: 214 layers, 7235389 parameters, 7235389 gradients, 16.6 GFLOPs\n",
            "\n",
            "Transferred 348/349 items from yolov5s.pt\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/herbert/Desktop/project/Kalman-Filter-with-Neural-networks/datasets/coco128/labels/train2017... 126 images, 2 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<00:00, 4668.40it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /home/herbert/Desktop/project/Kalman-Filter-with-Neural-networks/datasets/coco128/labels/train2017.cache\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.1GB ram): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<00:00, 2341.51it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/herbert/Desktop/project/Kalman-Filter-with-Neural-networks/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.1GB ram): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<00:00, 1576.30it/s]\n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m4.27 anchors/target, 0.994 Best Possible Recall (BPR). Current anchors are a good fit to dataset ‚úÖ\n",
            "Plotting labels to runs/train_QR/exp/labels.jpg... \n",
            "Image sizes 640 train, 640 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1mruns/train_QR/exp\u001b[0m\n",
            "Starting training for 3 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        0/2      3.26G    0.04505    0.06987    0.01727        207        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:04<00:00,  1.93it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/4 [00:00<?, ?it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/herbert/Desktop/project/Kalman-Filter-with-Neural-networks/yolov5/train.py\", line 844, in <module>\n",
            "    main(opt)\n",
            "  File \"/home/herbert/Desktop/project/Kalman-Filter-with-Neural-networks/yolov5/train.py\", line 619, in main\n",
            "    train(opt.hyp, opt, device, callbacks)\n",
            "  File \"/home/herbert/Desktop/project/Kalman-Filter-with-Neural-networks/yolov5/train.py\", line 426, in train\n",
            "    results, maps, _ = validate.run(\n",
            "  File \"/home/herbert/anaconda3/envs/d2l/lib/python3.9/site-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/herbert/Desktop/project/Kalman-Filter-with-Neural-networks/yolov5/val.py\", line 276, in run\n",
            "    correct = process_batch(predn, labelsn, iouv)\n",
            "  File \"/home/herbert/Desktop/project/Kalman-Filter-with-Neural-networks/yolov5/val.py\", line 105, in process_batch\n",
            "    iou = box_iou(labels[:, 1:], detections[:, :4])\n",
            "  File \"/home/herbert/Desktop/project/Kalman-Filter-with-Neural-networks/yolov5/utils/metrics.py\", line 292, in box_iou\n",
            "    inter = (torch.min(a2, b2) - torch.max(a1, b1)).clamp(0).prod(2)\n",
            "RuntimeError: \n",
            "  #define POS_INFINITY __int_as_float(0x7f800000)\n",
            "  #define INFINITY POS_INFINITY\n",
            "  #define NEG_INFINITY __int_as_float(0xff800000)\n",
            "  #define NAN __int_as_float(0x7fffffff)\n",
            "\n",
            "  typedef long long int int64_t;\n",
            "  typedef unsigned int uint32_t;\n",
            "  typedef signed char int8_t;\n",
            "  typedef unsigned char uint8_t;  // NOTE: this MUST be \"unsigned char\"! \"char\" is equivalent to \"signed char\"\n",
            "  typedef short int16_t;\n",
            "  static_assert(sizeof(int64_t) == 8, \"expected size does not match\");\n",
            "  static_assert(sizeof(uint32_t) == 4, \"expected size does not match\");\n",
            "  static_assert(sizeof(int8_t) == 1, \"expected size does not match\");\n",
            "  constexpr int num_threads = 128;\n",
            "  constexpr int thread_work_size = 4; // TODO: make template substitution once we decide where those vars live\n",
            "  constexpr int block_work_size = thread_work_size * num_threads;\n",
            "  //TODO use _assert_fail, because assert is disabled in non-debug builds\n",
            "  #define ERROR_UNSUPPORTED_CAST assert(false);\n",
            "\n",
            "  \n",
            "  \n",
            "  \n",
            "  namespace std {\n",
            "  \n",
            "  using ::signbit;\n",
            "  using ::isfinite;\n",
            "  using ::isinf;\n",
            "  using ::isnan;\n",
            "  \n",
            "  using ::abs;\n",
            "  \n",
            "  using ::acos;\n",
            "  using ::acosf;\n",
            "  using ::asin;\n",
            "  using ::asinf;\n",
            "  using ::atan;\n",
            "  using ::atanf;\n",
            "  using ::atan2;\n",
            "  using ::atan2f;\n",
            "  using ::ceil;\n",
            "  using ::ceilf;\n",
            "  using ::cos;\n",
            "  using ::cosf;\n",
            "  using ::cosh;\n",
            "  using ::coshf;\n",
            "  \n",
            "  using ::exp;\n",
            "  using ::expf;\n",
            "  \n",
            "  using ::fabs;\n",
            "  using ::fabsf;\n",
            "  using ::floor;\n",
            "  using ::floorf;\n",
            "  \n",
            "  using ::fmod;\n",
            "  using ::fmodf;\n",
            "  \n",
            "  using ::frexp;\n",
            "  using ::frexpf;\n",
            "  using ::ldexp;\n",
            "  using ::ldexpf;\n",
            "  \n",
            "  using ::log;\n",
            "  using ::logf;\n",
            "  \n",
            "  using ::log10;\n",
            "  using ::log10f;\n",
            "  using ::modf;\n",
            "  using ::modff;\n",
            "  \n",
            "  using ::pow;\n",
            "  using ::powf;\n",
            "  \n",
            "  using ::sin;\n",
            "  using ::sinf;\n",
            "  using ::sinh;\n",
            "  using ::sinhf;\n",
            "  \n",
            "  using ::sqrt;\n",
            "  using ::sqrtf;\n",
            "  using ::tan;\n",
            "  using ::tanf;\n",
            "  \n",
            "  using ::tanh;\n",
            "  using ::tanhf;\n",
            "  \n",
            "  using ::acosh;\n",
            "  using ::acoshf;\n",
            "  using ::asinh;\n",
            "  using ::asinhf;\n",
            "  using ::atanh;\n",
            "  using ::atanhf;\n",
            "  using ::cbrt;\n",
            "  using ::cbrtf;\n",
            "  \n",
            "  using ::copysign;\n",
            "  using ::copysignf;\n",
            "  \n",
            "  using ::erf;\n",
            "  using ::erff;\n",
            "  using ::erfc;\n",
            "  using ::erfcf;\n",
            "  using ::exp2;\n",
            "  using ::exp2f;\n",
            "  using ::expm1;\n",
            "  using ::expm1f;\n",
            "  using ::fdim;\n",
            "  using ::fdimf;\n",
            "  using ::fmaf;\n",
            "  using ::fma;\n",
            "  using ::fmax;\n",
            "  using ::fmaxf;\n",
            "  using ::fmin;\n",
            "  using ::fminf;\n",
            "  using ::hypot;\n",
            "  using ::hypotf;\n",
            "  using ::ilogb;\n",
            "  using ::ilogbf;\n",
            "  using ::lgamma;\n",
            "  using ::lgammaf;\n",
            "  using ::llrint;\n",
            "  using ::llrintf;\n",
            "  using ::llround;\n",
            "  using ::llroundf;\n",
            "  using ::log1p;\n",
            "  using ::log1pf;\n",
            "  using ::log2;\n",
            "  using ::log2f;\n",
            "  using ::logb;\n",
            "  using ::logbf;\n",
            "  using ::lrint;\n",
            "  using ::lrintf;\n",
            "  using ::lround;\n",
            "  using ::lroundf;\n",
            "  \n",
            "  using ::nan;\n",
            "  using ::nanf;\n",
            "  \n",
            "  using ::nearbyint;\n",
            "  using ::nearbyintf;\n",
            "  using ::nextafter;\n",
            "  using ::nextafterf;\n",
            "  using ::remainder;\n",
            "  using ::remainderf;\n",
            "  using ::remquo;\n",
            "  using ::remquof;\n",
            "  using ::rint;\n",
            "  using ::rintf;\n",
            "  using ::round;\n",
            "  using ::roundf;\n",
            "  using ::scalbln;\n",
            "  using ::scalblnf;\n",
            "  using ::scalbn;\n",
            "  using ::scalbnf;\n",
            "  using ::tgamma;\n",
            "  using ::tgammaf;\n",
            "  using ::trunc;\n",
            "  using ::truncf;\n",
            "  \n",
            "  } // namespace std\n",
            "  \n",
            "  \n",
            "\n",
            "  // NB: Order matters for this macro; it is relied upon in\n",
            "  // _promoteTypesLookup and the serialization format.\n",
            "  // Note, some types have ctype as void because we don't support them in codegen\n",
            "  #define AT_FORALL_SCALAR_TYPES_WITH_COMPLEX(_) \\\n",
            "  _(uint8_t, Byte) /* 0 */                               \\\n",
            "  _(int8_t, Char) /* 1 */                                \\\n",
            "  _(int16_t, Short) /* 2 */                              \\\n",
            "  _(int, Int) /* 3 */                                    \\\n",
            "  _(int64_t, Long) /* 4 */                               \\\n",
            "  _(at::Half, Half) /* 5 */                                  \\\n",
            "  _(float, Float) /* 6 */                                \\\n",
            "  _(double, Double) /* 7 */                              \\\n",
            "  _(std::complex<at::Half>, ComplexHalf) /* 8 */        \\\n",
            "  _(std::complex<float>, ComplexFloat) /* 9 */                          \\\n",
            "  _(std::complex<double>, ComplexDouble) /* 10 */                         \\\n",
            "  _(bool, Bool) /* 11 */                                 \\\n",
            "  _(void, QInt8) /* 12 */                          \\\n",
            "  _(void, QUInt8) /* 13 */                        \\\n",
            "  _(void, QInt32) /* 14 */                        \\\n",
            "  _(at::BFloat16, BFloat16) /* 15 */                             \\\n",
            "\n",
            "  #define AT_FORALL_SCALAR_TYPES_WITH_COMPLEX_EXCEPT_QINT(_)       \\\n",
            "  _(uint8_t, Byte)                                                 \\\n",
            "  _(int8_t, Char)                                                  \\\n",
            "  _(int16_t, Short)                                                \\\n",
            "  _(int, Int)                                                      \\\n",
            "  _(int64_t, Long)                                                 \\\n",
            "  _(at::Half, Half)                                                \\\n",
            "  _(float, Float)                                                  \\\n",
            "  _(double, Double)                                                \\\n",
            "  _(std::complex<at::Half>, ComplexHalf)                           \\\n",
            "  _(std::complex<float>, ComplexFloat)                             \\\n",
            "  _(std::complex<double>, ComplexDouble)                           \\\n",
            "  _(bool, Bool)                                                    \\\n",
            "  _(at::BFloat16, BFloat16)\n",
            "\n",
            "\n",
            "  enum class ScalarType : int8_t {\n",
            "  #define DEFINE_ENUM(_1, n) n,\n",
            "  AT_FORALL_SCALAR_TYPES_WITH_COMPLEX(DEFINE_ENUM)\n",
            "  #undef DEFINE_ENUM\n",
            "      Undefined,\n",
            "  NumOptions\n",
            "  };\n",
            "\n",
            "  template <typename T, int size>\n",
            "  struct Array {\n",
            "  T data[size];\n",
            "\n",
            "  __device__ T operator[](int i) const {\n",
            "      return data[i];\n",
            "  }\n",
            "  __device__ T& operator[](int i) {\n",
            "      return data[i];\n",
            "  }\n",
            "  Array() = default;\n",
            "  Array(const Array&) = default;\n",
            "  Array& operator=(const Array&) = default;\n",
            "  __device__ Array(T x) {\n",
            "    for (int i = 0; i < size; i++) {\n",
            "      data[i] = x;\n",
            "    }\n",
            "  }\n",
            "  };\n",
            "\n",
            "  \n",
            "  \n",
            "  \n",
            "  \n",
            "  \n",
            "\n",
            "\n",
            "\n",
            "  template <typename T>\n",
            "  struct DivMod {\n",
            "  T div;\n",
            "  T mod;\n",
            "\n",
            "  __device__ DivMod(T _div, T _mod) {\n",
            "      div = _div;\n",
            "      mod = _mod;\n",
            "  }\n",
            "  };\n",
            "\n",
            "  //<unsigned int>\n",
            "  struct IntDivider {\n",
            "  IntDivider() = default;\n",
            "\n",
            "  __device__ inline unsigned int div(unsigned int n) const {\n",
            "  unsigned int t = __umulhi(n, m1);\n",
            "  return (t + n) >> shift;\n",
            "  }\n",
            "\n",
            "  __device__ inline unsigned int mod(unsigned int n) const {\n",
            "  return n - div(n) * divisor;\n",
            "  }\n",
            "\n",
            "  __device__ inline DivMod<unsigned int> divmod(unsigned int n) const {\n",
            "  unsigned int q = div(n);\n",
            "  return DivMod<unsigned int>(q, n - q * divisor);\n",
            "  }\n",
            "\n",
            "  unsigned int divisor;  // d above.\n",
            "  unsigned int m1;  // Magic number: m' above.\n",
            "  unsigned int shift;  // Shift amounts.\n",
            "  };\n",
            "\n",
            "  template <int NARGS>\n",
            "  struct TrivialOffsetCalculator {\n",
            "    // The offset for each argument. Wrapper around fixed-size array.\n",
            "    // The offsets are in # of elements, not in bytes.\n",
            "    Array<unsigned int, NARGS> get(unsigned int linear_idx) const {\n",
            "      Array<unsigned int, NARGS> offsets;\n",
            "      #pragma unroll\n",
            "      for (int arg = 0; arg < NARGS; arg++) {\n",
            "        offsets[arg] = linear_idx;\n",
            "      }\n",
            "      return offsets;\n",
            "    }\n",
            "  };\n",
            "\n",
            "  template<int NARGS>\n",
            "  struct OffsetCalculator {\n",
            "  OffsetCalculator() = default;\n",
            "  __device__ __forceinline__ Array<unsigned int, NARGS> get(unsigned int linear_idx) const {\n",
            "      Array<unsigned int, NARGS> offsets;\n",
            "      #pragma unroll\n",
            "      for (int arg = 0; arg < NARGS; ++arg) {\n",
            "      offsets[arg] = 0;\n",
            "      }\n",
            "\n",
            "      #pragma unroll\n",
            "      for (int dim = 0; dim < 25; ++dim) {\n",
            "      if (dim == dims) {\n",
            "          break;\n",
            "      }\n",
            "\n",
            "      auto divmod = sizes_[dim].divmod(linear_idx);\n",
            "      linear_idx = divmod.div;\n",
            "\n",
            "      #pragma unroll\n",
            "      for (int arg = 0; arg < NARGS; ++arg) {\n",
            "          offsets[arg] += divmod.mod * strides_[dim][arg];\n",
            "      }\n",
            "      //printf(\"offset calc thread dim size stride offset %d %d %d %d %d %d %d %d\\n\",\n",
            "      //threadIdx.x, dim, sizes_[dim].divisor, strides_[dim][0], offsets[0], linear_idx, divmod.div, divmod.mod);\n",
            "      }\n",
            "      return offsets;\n",
            "  }\n",
            "\n",
            "    int dims;\n",
            "    IntDivider sizes_[25];\n",
            "    // NOTE: this approach will not support nInputs == 0\n",
            "    unsigned int strides_[25][NARGS];\n",
            "  };\n",
            "\n",
            "\n",
            "\n",
            "  #define C10_HOST_DEVICE __host__ __device__\n",
            "  #define C10_DEVICE __device__\n",
            "\n",
            "  template <typename T>\n",
            "  __device__ __forceinline__ T WARP_SHFL_DOWN(T value, unsigned int delta, int width = warpSize, unsigned int mask = 0xffffffff)\n",
            "  {\n",
            "    return __shfl_down_sync(mask, value, delta, width);\n",
            "  }\n",
            "\n",
            "\n",
            "  #if 0\n",
            "  template <typename T>\n",
            "  __device__ __forceinline__ std::complex<T> WARP_SHFL_DOWN(std::complex<T> value, unsigned int delta, int width = warpSize, unsigned int mask = 0xffffffff)\n",
            "  {\n",
            "    return std::complex<T>(\n",
            "        __shfl_down_sync(mask, value.real(), delta, width),\n",
            "        __shfl_down_sync(mask, value.imag(), delta, width));\n",
            "  }\n",
            "  #endif\n",
            "\n",
            "  // aligned vector generates vectorized load/store on CUDA\n",
            "  template<typename scalar_t, int vec_size>\n",
            "  struct alignas(sizeof(scalar_t) * vec_size) aligned_vector {\n",
            "    scalar_t val[vec_size];\n",
            "  };\n",
            "\n",
            "\n",
            "  C10_HOST_DEVICE static void reduce_fraction(size_t &numerator, size_t &denominator) {\n",
            "    // get GCD of num and denom using Euclid's algorithm.\n",
            "    // Can replace this with std::gcd if we ever support c++17.\n",
            "    size_t a = denominator;\n",
            "    size_t b = numerator;\n",
            "    while (b != 0) {\n",
            "        a %= b;\n",
            "        // swap(a,b)\n",
            "        size_t tmp = a;\n",
            "        a = b;\n",
            "        b = tmp;\n",
            "    }\n",
            "\n",
            "    // a is now the GCD\n",
            "    numerator /= a;\n",
            "    denominator /= a;\n",
            "  }\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  struct ReduceConfig {\n",
            "  //has to match host-side ReduceConfig in the eager code\n",
            "  static constexpr int BLOCK_X = 0;\n",
            "  static constexpr int BLOCK_Y = 1;\n",
            "  static constexpr int CTA = 2;\n",
            "\n",
            "  static constexpr int input_vec_size = 4;\n",
            "  int element_size_bytes;\n",
            "  int num_inputs;\n",
            "  int num_outputs;\n",
            "  int step_input = 1;\n",
            "  int step_output = 1;\n",
            "  int ctas_per_output = 1;\n",
            "  int input_mult[3] = {0, 0, 0};\n",
            "  int output_mult[2] = {0, 0};\n",
            "\n",
            "  int block_width;\n",
            "  int block_height;\n",
            "  int num_threads;\n",
            "\n",
            "  bool vectorize_input = false;\n",
            "  int output_vec_size = 1;\n",
            "\n",
            "  C10_HOST_DEVICE bool should_block_x_reduce() const {\n",
            "    return input_mult[BLOCK_X] != 0;\n",
            "  }\n",
            "\n",
            "  C10_HOST_DEVICE bool should_block_y_reduce() const {\n",
            "    return input_mult[BLOCK_Y] != 0;\n",
            "  }\n",
            "\n",
            "  C10_HOST_DEVICE bool should_global_reduce() const {\n",
            "    return input_mult[CTA] != 0;\n",
            "  }\n",
            "\n",
            "  C10_DEVICE bool should_store(int output_idx) const {\n",
            "    return output_idx < num_outputs &&\n",
            "      (!should_block_x_reduce() || threadIdx.x == 0) &&\n",
            "      (!should_block_y_reduce() || threadIdx.y == 0);\n",
            "  }\n",
            "\n",
            "  C10_DEVICE bool should_reduce_tail() const {\n",
            "    return (!should_block_y_reduce() || threadIdx.y == 0) &&\n",
            "      (!should_global_reduce() || blockIdx.y == 0);\n",
            "  }\n",
            "\n",
            "  C10_HOST_DEVICE int input_idx() const {\n",
            "    int lane = threadIdx.x;\n",
            "    int warp = threadIdx.y;\n",
            "    int cta2 = blockIdx.y;\n",
            "    return (lane * input_mult[BLOCK_X] +\n",
            "            warp * input_mult[BLOCK_Y] +\n",
            "            cta2 * input_mult[CTA]);\n",
            "  }\n",
            "\n",
            "  template <int output_vec_size>\n",
            "  C10_HOST_DEVICE int output_idx() const {\n",
            "    int lane = threadIdx.x;\n",
            "    int warp = threadIdx.y;\n",
            "    int cta1 = blockIdx.x;\n",
            "    return (lane * output_mult[BLOCK_X] +\n",
            "            warp * output_mult[BLOCK_Y] +\n",
            "            cta1 * step_output) * output_vec_size;\n",
            "  }\n",
            "\n",
            "  C10_DEVICE int shared_memory_offset(int offset) const {\n",
            "    return threadIdx.x + (threadIdx.y + offset) * blockDim.x;\n",
            "  }\n",
            "\n",
            "  C10_DEVICE int staging_memory_offset(int cta2) const {\n",
            "    int offset = cta2 + blockIdx.x * gridDim.y;\n",
            "    if (!should_block_x_reduce()) {\n",
            "      offset = threadIdx.x + offset * blockDim.x;\n",
            "    }\n",
            "    return offset;\n",
            "  }\n",
            "\n",
            "\n",
            "  };\n",
            "\n",
            "\n",
            "//TODO this will need to be different for more generic reduction functions\n",
            "namespace reducer {\n",
            "\n",
            "  using scalar_t = float;\n",
            "  using arg_t = float;\n",
            "  using out_scalar_t = float;\n",
            "\n",
            "\n",
            "  inline __device__ arg_t combine(arg_t a, arg_t b) { return a * b; }\n",
            "\n",
            "  inline __device__ out_scalar_t project(arg_t arg) {\n",
            "    return (out_scalar_t) arg;\n",
            "  }\n",
            "\n",
            "  inline __device__ arg_t warp_shfl_down(arg_t arg, int offset) {\n",
            "    return WARP_SHFL_DOWN(arg, offset);\n",
            "  }\n",
            "\n",
            "  inline __device__ arg_t translate_idx(arg_t acc, int64_t /*idx*/) {\n",
            "    return acc;\n",
            "  }\n",
            "\n",
            "  // wrap a normal reduction that ignores the index\n",
            "  inline __device__ arg_t reduce(arg_t acc, arg_t val, int64_t idx) {\n",
            "     return combine(acc, val);\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "struct ReduceJitOp {\n",
            "  using scalar_t = float;\n",
            "  using arg_t = float;\n",
            "  using out_scalar_t = float;\n",
            "\n",
            "  using InputCalculator = OffsetCalculator<1>;\n",
            "  using OutputCalculator = OffsetCalculator<2>;\n",
            "\n",
            "//   static constexpr bool can_accumulate_in_output =\n",
            "//     std::is_convertible<arg_t, out_scalar_t>::value\n",
            "//     && std::is_convertible<out_scalar_t, arg_t>::value;\n",
            "\n",
            "  static constexpr int input_vec_size = ReduceConfig::input_vec_size;\n",
            "\n",
            "  arg_t ident;\n",
            "  ReduceConfig config;\n",
            "  InputCalculator input_calc;\n",
            "  OutputCalculator output_calc;\n",
            "  const void* src;\n",
            "  const char* dst[2]; //it accepts at most two destinations\n",
            "  // acc_buf used for accumulation among sub Tensor Iterator when accumulation on\n",
            "  // output is not permissible\n",
            "  void* acc_buf;\n",
            "  // cta_buf used for accumulation between blocks during global reduction\n",
            "  void* cta_buf;\n",
            "  int* semaphores;\n",
            "  int64_t base_idx;\n",
            "  bool accumulate;\n",
            "  bool final_output;\n",
            "  int noutputs;\n",
            "\n",
            "\n",
            "  C10_DEVICE void run() const {\n",
            "    extern __shared__ char shared_memory[];\n",
            "    uint32_t output_idx = config.output_idx<1>();\n",
            "    uint32_t input_idx = config.input_idx();\n",
            "    auto base_offsets1 = output_calc.get(output_idx)[1];\n",
            "\n",
            "    using arg_vec_t = Array<arg_t, 1>;\n",
            "    arg_vec_t value;\n",
            "\n",
            "    if (output_idx < config.num_outputs && input_idx < config.num_inputs) {\n",
            "      const scalar_t* input_slice = (const scalar_t*)((const char*)src + base_offsets1);\n",
            "\n",
            "      value = thread_reduce<1>(input_slice);\n",
            "    }\n",
            "\n",
            "    if (config.should_block_y_reduce()) {\n",
            "      value = block_y_reduce<1>(value, shared_memory);\n",
            "    }\n",
            "    if (config.should_block_x_reduce()) {\n",
            "      value = block_x_reduce<1>(value, shared_memory);\n",
            "    }\n",
            "\n",
            "    using out_ptr_vec_t = Array<out_scalar_t*, 1>;\n",
            "    using offset_vec_t = Array<uint32_t, 1>;\n",
            "    offset_vec_t base_offsets;\n",
            "    out_ptr_vec_t out;\n",
            "\n",
            "    #pragma unroll\n",
            "    for (int i = 0; i < 1; i++) {\n",
            "      base_offsets[i] = output_calc.get(output_idx + i)[0];\n",
            "      out[i] = (out_scalar_t*)((char*)dst[0] + base_offsets[i]);\n",
            "    }\n",
            "\n",
            "    arg_vec_t* acc = nullptr;\n",
            "    if (acc_buf != nullptr) {\n",
            "      size_t numerator = sizeof(arg_t);\n",
            "      size_t denominator = sizeof(out_scalar_t);\n",
            "      reduce_fraction(numerator, denominator);\n",
            "      acc = (arg_vec_t*)((char*)acc_buf + (base_offsets[0] * numerator / denominator));\n",
            "    }\n",
            "\n",
            "    if (config.should_global_reduce()) {\n",
            "      value = global_reduce<1>(value, acc, shared_memory);\n",
            "    } else if (config.should_store(output_idx)) {\n",
            "      if (accumulate) {\n",
            "        #pragma unroll\n",
            "        for (int i = 0; i < 1; i++) {\n",
            "          value[i] = reducer::translate_idx(value[i], base_idx);\n",
            "        }\n",
            "      }\n",
            "\n",
            "      if (acc == nullptr) {\n",
            "        if (accumulate) {\n",
            "          value = accumulate_in_output<1>(out, value);\n",
            "        }\n",
            "        if (final_output) {\n",
            "          set_results_to_output<1>(value, base_offsets);\n",
            "        } else {\n",
            "          #pragma unroll\n",
            "          for (int i = 0; i < 1; i++) {\n",
            "            *(out[i]) = get_accumulated_output(out[i], value[i]);\n",
            "          }\n",
            "        }\n",
            "      } else {\n",
            "        if (accumulate) {\n",
            "          #pragma unroll\n",
            "          for (int i = 0; i < 1; i++) {\n",
            "            value[i] = reducer::combine((*acc)[i], value[i]);\n",
            "          }\n",
            "        }\n",
            "        if (final_output) {\n",
            "          set_results_to_output<1>(value, base_offsets);\n",
            "        } else {\n",
            "          *acc = value;\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "\n",
            "  template <int output_vec_size>\n",
            "  C10_DEVICE Array<arg_t, output_vec_size> thread_reduce(const scalar_t* data) const {\n",
            "    if (config.vectorize_input) {\n",
            "      assert(output_vec_size == 1);\n",
            "      // reduce at the header of input_slice where memory is not aligned,\n",
            "      // so that thread_reduce will have an aligned memory to work on.\n",
            "      return {input_vectorized_thread_reduce_impl(data)};\n",
            "    } else {\n",
            "      uint32_t element_stride = input_calc.strides_[0][0] / sizeof(scalar_t);\n",
            "      bool is_contiguous = (input_calc.dims == 1 && element_stride == 1);\n",
            "      if (is_contiguous) {\n",
            "        return thread_reduce_impl<output_vec_size>(data, [](uint32_t idx) { return idx; });\n",
            "      } else if (input_calc.dims == 1) {\n",
            "        return thread_reduce_impl<output_vec_size>(data, [&](uint32_t idx) { return idx * element_stride; });\n",
            "      } else {\n",
            "        return thread_reduce_impl<output_vec_size>(data, [&](uint32_t idx) { return input_calc.get(idx)[0] / sizeof(scalar_t); });\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "\n",
            "  C10_DEVICE arg_t input_vectorized_thread_reduce_impl(const scalar_t* data) const {\n",
            "    uint32_t end = config.num_inputs;\n",
            "\n",
            "    // Handle the head of input slice where data is not aligned\n",
            "    arg_t value = ident;\n",
            "    constexpr int align_bytes = alignof(aligned_vector<scalar_t, input_vec_size>);\n",
            "    constexpr int align_elements = align_bytes / sizeof(scalar_t);\n",
            "    int shift = ((int64_t)data) % align_bytes / sizeof(scalar_t);\n",
            "    if (shift > 0) {\n",
            "      data -= shift;\n",
            "      end += shift;\n",
            "      if(threadIdx.x >= shift && threadIdx.x < align_elements && config.should_reduce_tail()){\n",
            "        value = reducer::reduce(value, data[threadIdx.x], threadIdx.x - shift);\n",
            "      }\n",
            "      end -= align_elements;\n",
            "      data += align_elements;\n",
            "      shift = align_elements - shift;\n",
            "    }\n",
            "\n",
            "    // Do the vectorized reduction\n",
            "    using load_t = aligned_vector<scalar_t, input_vec_size>;\n",
            "\n",
            "    uint32_t idx = config.input_idx();\n",
            "    const uint32_t stride = config.step_input;\n",
            "\n",
            "    // Multiple accumulators to remove dependency between unrolled loops.\n",
            "    arg_t value_list[input_vec_size];\n",
            "    value_list[0] = value;\n",
            "\n",
            "    #pragma unroll\n",
            "    for (int i = 1; i < input_vec_size; i++) {\n",
            "      value_list[i] = ident;\n",
            "    }\n",
            "\n",
            "    scalar_t values[input_vec_size];\n",
            "\n",
            "    load_t *values_vector = reinterpret_cast<load_t*>(&values[0]);\n",
            "\n",
            "    while (idx * input_vec_size + input_vec_size - 1 < end) {\n",
            "      *values_vector = reinterpret_cast<const load_t*>(data)[idx];\n",
            "      #pragma unroll\n",
            "      for (uint32_t i = 0; i < input_vec_size; i++) {\n",
            "        value_list[i] = reducer::reduce(value_list[i], values[i], shift + idx * input_vec_size + i);\n",
            "      }\n",
            "      idx += stride;\n",
            "    }\n",
            "\n",
            "    // tail\n",
            "    uint32_t tail_start = end - end % input_vec_size;\n",
            "    if (config.should_reduce_tail()) {\n",
            "      int idx = tail_start + threadIdx.x;\n",
            "      if (idx < end) {\n",
            "        value_list[0] = reducer::reduce(value_list[0], data[idx], idx + shift);\n",
            "      }\n",
            "    }\n",
            "\n",
            "    // combine accumulators\n",
            "    #pragma unroll\n",
            "    for (int i = 1; i < input_vec_size; i++) {\n",
            "      value_list[0] = reducer::combine(value_list[0], value_list[i]);\n",
            "    }\n",
            "    return value_list[0];\n",
            "  }\n",
            "\n",
            "  template <int output_vec_size, typename offset_calc_t>\n",
            "  C10_DEVICE Array<arg_t, output_vec_size> thread_reduce_impl(const scalar_t* data_, offset_calc_t calc) const {\n",
            "    uint32_t idx = config.input_idx();\n",
            "    const uint32_t end = config.num_inputs;\n",
            "    const uint32_t stride = config.step_input;\n",
            "    const int vt0=4;\n",
            "\n",
            "    using arg_vec_t = Array<arg_t, output_vec_size>;\n",
            "    using load_t = aligned_vector<scalar_t, output_vec_size>;\n",
            "    const load_t* data = reinterpret_cast<const load_t*>(data_);\n",
            "\n",
            "    // Multiple accumulators to remove dependency between unrolled loops.\n",
            "    arg_vec_t value_list[vt0];\n",
            "\n",
            "    #pragma unroll\n",
            "    for (int i = 0; i < vt0; i++) {\n",
            "      #pragma unroll\n",
            "      for (int j = 0; j < output_vec_size; j++) {\n",
            "        value_list[i][j] = ident;\n",
            "      }\n",
            "    }\n",
            "\n",
            "    load_t values[vt0];\n",
            "\n",
            "    while (idx + (vt0 - 1) * stride < end) {\n",
            "      #pragma unroll\n",
            "      for (uint32_t i = 0; i < vt0; i++) {\n",
            "        values[i] = data[calc(idx + i * stride) / output_vec_size];\n",
            "      }\n",
            "      #pragma unroll\n",
            "      for (uint32_t i = 0; i < vt0; i++) {\n",
            "        #pragma unroll\n",
            "        for (uint32_t j = 0; j < output_vec_size; j++) {\n",
            "          value_list[i][j] = reducer::reduce(value_list[i][j], values[i].val[j], idx + i * stride);\n",
            "        }\n",
            "      }\n",
            "      idx += stride * vt0;\n",
            "    }\n",
            "\n",
            "    // tail\n",
            "    int idx_ = idx;\n",
            "    #pragma unroll\n",
            "    for (uint32_t i = 0; i < vt0; i++) {\n",
            "      if (idx >= end) {\n",
            "        break;\n",
            "      }\n",
            "      values[i] = data[calc(idx) / output_vec_size];\n",
            "      idx += stride;\n",
            "    }\n",
            "    idx = idx_;\n",
            "    #pragma unroll\n",
            "    for (uint32_t i = 0; i < vt0; i++) {\n",
            "      if (idx >= end) {\n",
            "        break;\n",
            "      }\n",
            "      #pragma unroll\n",
            "      for (uint32_t j = 0; j < output_vec_size; j++) {\n",
            "        value_list[i][j] = reducer::reduce(value_list[i][j], values[i].val[j], idx);\n",
            "      }\n",
            "      idx += stride;\n",
            "    }\n",
            "\n",
            "    // combine accumulators\n",
            "    #pragma unroll\n",
            "    for (int i = 1; i < vt0; i++) {\n",
            "      #pragma unroll\n",
            "      for (uint32_t j = 0; j < output_vec_size; j++) {\n",
            "        value_list[0][j] = reducer::combine(value_list[0][j], value_list[i][j]);\n",
            "      }\n",
            "    }\n",
            "    return value_list[0];\n",
            "  }\n",
            "  template <int output_vec_size>\n",
            "  C10_DEVICE Array<arg_t, output_vec_size> block_x_reduce(Array<arg_t, output_vec_size> value, char* shared_memory) const {\n",
            "    using args_vec_t = Array<arg_t, output_vec_size>;\n",
            "    int dim_x = blockDim.x;\n",
            "    args_vec_t* shared = (args_vec_t*)shared_memory;\n",
            "    if (dim_x > warpSize) {\n",
            "      int address_base = threadIdx.x + threadIdx.y*blockDim.x;\n",
            "      shared[address_base] = value;\n",
            "      for (int offset = dim_x/2; offset >= warpSize; offset >>= 1) {\n",
            "        __syncthreads();\n",
            "        if (threadIdx.x < offset && threadIdx.x + offset < blockDim.x) {\n",
            "          args_vec_t other = shared[address_base + offset];\n",
            "          #pragma unroll\n",
            "          for (int i = 0; i < output_vec_size; i++) {\n",
            "            value[i] = reducer::combine(value[i], other[i]);\n",
            "          }\n",
            "          shared[address_base] = value;\n",
            "        }\n",
            "      }\n",
            "      dim_x = warpSize;\n",
            "    }\n",
            "\n",
            "    __syncthreads();\n",
            "\n",
            "    for (int offset = 1; offset < dim_x; offset <<= 1) {\n",
            "      #pragma unroll\n",
            "      for (int i = 0; i < output_vec_size; i++) {\n",
            "        arg_t other = reducer::warp_shfl_down(value[i], offset);\n",
            "        value[i] = reducer::combine(value[i], other);\n",
            "      }\n",
            "    }\n",
            "    return value;\n",
            "  }\n",
            "\n",
            "  template <int output_vec_size>\n",
            "  C10_DEVICE Array<arg_t, output_vec_size> block_y_reduce(Array<arg_t, output_vec_size> value, char* shared_memory) const {\n",
            "    using args_vec_t = Array<arg_t, output_vec_size>;\n",
            "    args_vec_t* shared = (args_vec_t*)shared_memory;\n",
            "    shared[config.shared_memory_offset(0)] = value;\n",
            "    for (int offset = blockDim.y / 2; offset > 0; offset >>= 1) {\n",
            "      __syncthreads();\n",
            "      if (threadIdx.y < offset && threadIdx.y + offset < blockDim.y) {\n",
            "        args_vec_t other = shared[config.shared_memory_offset(offset)];\n",
            "        #pragma unroll\n",
            "        for (int i = 0; i < output_vec_size; i++) {\n",
            "          value[i] = reducer::combine(value[i], other[i]);\n",
            "        }\n",
            "        shared[config.shared_memory_offset(0)] = value;\n",
            "      }\n",
            "    }\n",
            "    return value;\n",
            "  }\n",
            "  \n",
            "\n",
            "  C10_DEVICE bool mark_block_finished() const {\n",
            "    __shared__ bool is_last_block_done_shared;\n",
            "\n",
            "    __syncthreads();\n",
            "    if (threadIdx.x == 0 && threadIdx.y == 0) {\n",
            "      int prev_blocks_finished = atomicAdd(&semaphores[blockIdx.x], 1);\n",
            "      is_last_block_done_shared = (prev_blocks_finished == gridDim.y - 1);\n",
            "    }\n",
            "\n",
            "    __syncthreads();\n",
            "\n",
            "    return is_last_block_done_shared;\n",
            "  }\n",
            "\n",
            "  template <int output_vec_size>\n",
            "  C10_DEVICE Array<arg_t, output_vec_size> accumulate_in_output(\n",
            "    Array<out_scalar_t*, output_vec_size> out,\n",
            "    Array<arg_t, output_vec_size> value\n",
            "  ) const {\n",
            "    Array<arg_t, output_vec_size> ret;\n",
            "    #pragma unroll\n",
            "    for (int i = 0; i < output_vec_size; i++) {\n",
            "      ret[i] = reducer::combine(*(out[i]), value[i]);\n",
            "    }\n",
            "    return ret;\n",
            "  }\n",
            "\n",
            "\n",
            "  C10_DEVICE out_scalar_t get_accumulated_output(\n",
            "    out_scalar_t* out, arg_t value\n",
            "  ) const {\n",
            "    assert(!final_output);\n",
            "    return (out_scalar_t)value;\n",
            "  }\n",
            "\n",
            "  template<class T>\n",
            "  C10_DEVICE void set_results(const T x, const uint32_t base_offset) const {\n",
            "    assert(noutputs == 1);\n",
            "    auto res = (out_scalar_t*)((char*)dst[0] + base_offset);\n",
            "    *res = x;\n",
            "  }\n",
            "\n",
            "//TODO - multi-output reduction - we won't be able to use thrust::pair\n",
            "//just explicitly specify typed output reads/writes\n",
            "//Currently implemented for max of two outputs\n",
            "//   template<class T1, class T2>\n",
            "//   C10_DEVICE void set_results(const thrust::pair<T1, T2> x, const index_t base_offset) const {\n",
            "//     if (noutputs >= 1) {\n",
            "//       auto res0 = (T1*)((char*)dst[0] + base_offset);\n",
            "//       *res0 = x.first;\n",
            "//     }\n",
            "//     if (noutputs >= 2) {\n",
            "//       // base offset is computed assuming element size being sizeof(T1), so we need to make a\n",
            "//       // correction to obtain the correct base offset\n",
            "//       auto res1 = (T2*) ((char *) dst[1] + base_offset / sizeof(T1) * sizeof(T2));\n",
            "//       *res1 = x.second;\n",
            "//     }\n",
            "//   }\n",
            "\n",
            "  template <int output_vec_size>\n",
            "  C10_DEVICE void set_results_to_output(Array<arg_t, output_vec_size> value, Array<uint32_t, output_vec_size> base_offset) const {\n",
            "    assert(final_output);\n",
            "    #pragma unroll\n",
            "    for (int i = 0; i < output_vec_size; i++) {\n",
            "      set_results(reducer::project(value[i]), base_offset[i]);\n",
            "    }\n",
            "  }\n",
            "\n",
            "  template <int output_vec_size>\n",
            "  C10_DEVICE Array<arg_t, output_vec_size> global_reduce(Array<arg_t, output_vec_size> value, Array<arg_t, output_vec_size> *acc, char* shared_memory) const {\n",
            "    using arg_vec_t = Array<arg_t, output_vec_size>;\n",
            "    using out_ptr_vec_t = Array<out_scalar_t*, output_vec_size>;\n",
            "    using offset_vec_t = Array<uint32_t, output_vec_size>;\n",
            "\n",
            "    arg_vec_t* reduce_buffer = (arg_vec_t*)cta_buf;\n",
            "    uint32_t output_idx = config.output_idx<output_vec_size>();\n",
            "    offset_vec_t base_offsets;\n",
            "    out_ptr_vec_t out;\n",
            "\n",
            "    #pragma unroll\n",
            "    for (int i = 0; i < output_vec_size; i++) {\n",
            "      base_offsets[i] = output_calc.get(output_idx + i)[0];\n",
            "      out[i] = (out_scalar_t*)((char*)dst[0] + base_offsets[i]);\n",
            "    }\n",
            "\n",
            "    bool should_store = config.should_store(output_idx);\n",
            "    if (should_store) {\n",
            "      uint32_t offset = config.staging_memory_offset(blockIdx.y);\n",
            "      reduce_buffer[offset] = value;\n",
            "    }\n",
            "\n",
            "    __threadfence(); // make sure writes are globally visible\n",
            "    __syncthreads(); // if multiple warps in this block wrote to staging, make sure they're all done\n",
            "    bool is_last_block_done = mark_block_finished();\n",
            "\n",
            "    if (is_last_block_done) {\n",
            "      value = ident;\n",
            "      if (config.should_block_x_reduce()) {\n",
            "        uint32_t input_offset = threadIdx.x + threadIdx.y * blockDim.x;\n",
            "        uint32_t step = blockDim.x * blockDim.y;\n",
            "        for (; input_offset < config.ctas_per_output; input_offset += step) {\n",
            "          uint32_t idx = config.staging_memory_offset(input_offset);\n",
            "          arg_vec_t next = reduce_buffer[idx];\n",
            "          #pragma unroll\n",
            "          for (int i = 0; i < output_vec_size; i++) {\n",
            "            value[i] = reducer::combine(value[i], next[i]);\n",
            "          }\n",
            "        }\n",
            "      } else {\n",
            "        uint32_t input_offset = threadIdx.y;\n",
            "        uint32_t step = blockDim.y;\n",
            "        for (; input_offset < config.ctas_per_output; input_offset += step) {\n",
            "          uint32_t idx = config.staging_memory_offset(input_offset);\n",
            "          arg_vec_t next = reduce_buffer[idx];\n",
            "          #pragma unroll\n",
            "          for (int i = 0; i < output_vec_size; i++) {\n",
            "            value[i] = reducer::combine(value[i], next[i]);\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "      value = block_y_reduce(value, shared_memory);\n",
            "      if (config.should_block_x_reduce()) {\n",
            "        value = block_x_reduce<output_vec_size>(value, shared_memory);\n",
            "      }\n",
            "      if (should_store) {\n",
            "        if (accumulate) {\n",
            "          #pragma unroll\n",
            "          for (int i = 0; i < output_vec_size; i++) {\n",
            "            value[i] = reducer::translate_idx(value[i], base_idx);\n",
            "          }\n",
            "        }\n",
            "\n",
            "        if (acc == nullptr) {\n",
            "          if (accumulate) {\n",
            "            value = accumulate_in_output<output_vec_size>(out, value);\n",
            "          }\n",
            "          if (final_output) {\n",
            "            set_results_to_output<output_vec_size>(value, base_offsets);\n",
            "          } else {\n",
            "            #pragma unroll\n",
            "            for (int i = 0; i < output_vec_size; i++) {\n",
            "              *(out[i]) = get_accumulated_output(out[i], value[i]);\n",
            "            }\n",
            "          }\n",
            "        } else {\n",
            "          if (accumulate) {\n",
            "            #pragma unroll\n",
            "            for (int i = 0; i < output_vec_size; i++) {\n",
            "              value[i] = reducer::combine((*acc)[i], value[i]);\n",
            "            }\n",
            "          }\n",
            "          if (final_output) {\n",
            "            set_results_to_output<output_vec_size>(value, base_offsets);\n",
            "          } else {\n",
            "            *acc = value;\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "\n",
            "    return value;\n",
            "  }\n",
            "};\n",
            "\n",
            "extern \"C\"\n",
            "__launch_bounds__(512, 4)\n",
            "__global__ void reduction_prod_kernel(ReduceJitOp r){\n",
            "  r.run();\n",
            "}\n",
            "nvrtc: error: invalid value for --gpu-architecture (-arch)\n",
            "\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml OfflineExperiment Summary\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : exp\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : [OfflineExperiment will get URL after upload]\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     loss : 1.883742332458496\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Name                        : exp\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     comet_log_batch_metrics     : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     comet_log_confusion_matrix  : True\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     comet_log_per_class_metrics : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     comet_max_image_uploads     : 100\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     comet_mode                  : online\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     comet_model_name            : yolov5\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hasNestedParams             : True\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     offline_experiment          : True\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     anchor_t            : 4.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     artifact_alias      : latest\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     batch_size          : 16\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     bbox_interval       : -1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     box                 : 0.05\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     bucket              : \n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     cfg                 : ./models/yolov5s_qrcode.yaml\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     cls                 : 0.5\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     cls_pw              : 1.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     copy_paste          : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     cos_lr              : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     degrees             : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     device              : \n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     entity              : None\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     evolve              : None\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     exist_ok            : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     fl_gamma            : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     fliplr              : 0.5\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     flipud              : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     freeze              : [0]\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hsv_h               : 0.015\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hsv_s               : 0.7\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hsv_v               : 0.4\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|anchor_t        : 4.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|box             : 0.05\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|cls             : 0.5\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|cls_pw          : 1.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|copy_paste      : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|degrees         : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|fl_gamma        : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|fliplr          : 0.5\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|flipud          : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|hsv_h           : 0.015\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|hsv_s           : 0.7\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|hsv_v           : 0.4\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|iou_t           : 0.2\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|lr0             : 0.01\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|lrf             : 0.01\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|mixup           : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|momentum        : 0.937\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|mosaic          : 1.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|obj             : 1.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|obj_pw          : 1.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|perspective     : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|scale           : 0.5\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|shear           : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|translate       : 0.1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|warmup_bias_lr  : 0.1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|warmup_epochs   : 3.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|warmup_momentum : 0.8\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hyp|weight_decay    : 0.0005\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     image_weights       : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     imgsz               : 640\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     iou_t               : 0.2\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     label_smoothing     : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     local_rank          : -1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lr0                 : 0.01\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lrf                 : 0.01\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mixup               : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     momentum            : 0.937\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mosaic              : 1.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     multi_scale         : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                : exp\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     ndjson_console      : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     ndjson_file         : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     noautoanchor        : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     noplots             : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     nosave              : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     noval               : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     obj                 : 1.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     obj_pw              : 1.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     optimizer           : SGD\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     patience            : 100\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     perspective         : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     project             : runs/train_QR\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     quad                : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     rect                : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     resume              : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     save_dir            : runs/train_QR/exp\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     save_period         : -1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     scale               : 0.5\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     seed                : 0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     shear               : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     single_cls          : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     sync_bn             : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     translate           : 0.1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     upload_dataset      : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val_conf_threshold  : 0.001\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val_iou_threshold   : 0.6\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     warmup_bias_lr      : 0.1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     warmup_epochs       : 3.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     warmup_momentum     : 0.8\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     weight_decay        : 0.0005\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     workers             : 0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     asset                        : 5 (449.46 KB)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-environment-definition : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-info                   : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-specification          : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details          : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git metadata                 : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages           : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model graph                  : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     os packages                  : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still saving offline stats to messages file before program termination (may take up to 120 seconds)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Starting saving the offline archive\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m To upload this offline experiment, run:\n",
            "    comet upload /home/herbert/Desktop/project/Kalman-Filter-with-Neural-networks/yolov5/.cometml-runs/b68c4319346f4ee3b9364e9bf334e91c.zip\n"
          ]
        }
      ],
      "source": [
        "# Train YOLOv5s on COCO128 for 3 epochs\n",
        "!python train.py --img 640 --batch 16 --epochs 3 --data coco128.yaml --weights yolov5s.pt --cache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15glLzbQx5u0"
      },
      "source": [
        "# 4. Visualize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWOsI5wJR1o3"
      },
      "source": [
        "## Comet Logging and Visualization üåü NEW\n",
        "\n",
        "[Comet](https://www.comet.com/site/lp/yolov5-with-comet/?utm_source=yolov5&utm_medium=partner&utm_campaign=partner_yolov5_2022&utm_content=yolov5_colab) is now fully integrated with YOLOv5. Track and visualize model metrics in real time, save your hyperparameters, datasets, and model checkpoints, and visualize your model predictions with [Comet Custom Panels](https://www.comet.com/docs/v2/guides/comet-dashboard/code-panels/about-panels/?utm_source=yolov5&utm_medium=partner&utm_campaign=partner_yolov5_2022&utm_content=yolov5_colab)! Comet makes sure you never lose track of your work and makes it easy to share results and collaborate across teams of all sizes!\n",
        "\n",
        "Getting started is easy:\n",
        "```shell\n",
        "pip install comet_ml  # 1. install\n",
        "export COMET_API_KEY=<Your API Key>  # 2. paste API key\n",
        "python train.py --img 640 --epochs 3 --data coco128.yaml --weights yolov5s.pt  # 3. train\n",
        "```\n",
        "To learn more about all of the supported Comet features for this integration, check out the [Comet Tutorial](https://docs.ultralytics.com/yolov5/tutorials/comet_logging_integration). If you'd like to learn more about Comet, head over to our [documentation](https://www.comet.com/docs/v2/?utm_source=yolov5&utm_medium=partner&utm_campaign=partner_yolov5_2022&utm_content=yolov5_colab). Get started by trying out the Comet Colab Notebook:\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1RG0WOQyxlDlo5Km8GogJpIEJlg_5lyYO?usp=sharing)\n",
        "\n",
        "<a href=\"https://bit.ly/yolov5-readme-comet2\">\n",
        "<img alt=\"Comet Dashboard\" src=\"https://user-images.githubusercontent.com/26833433/202851203-164e94e1-2238-46dd-91f8-de020e9d6b41.png\" width=\"1280\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lay2WsTjNJzP"
      },
      "source": [
        "## ClearML Logging and Automation üåü NEW\n",
        "\n",
        "[ClearML](https://cutt.ly/yolov5-notebook-clearml) is completely integrated into YOLOv5 to track your experimentation, manage dataset versions and even remotely execute training runs. To enable ClearML (check cells above):\n",
        "\n",
        "- `pip install clearml`\n",
        "- run `clearml-init` to connect to a ClearML server (**deploy your own [open-source server](https://github.com/allegroai/clearml-server)**, or use our [free hosted server](https://cutt.ly/yolov5-notebook-clearml))\n",
        "\n",
        "You'll get all the great expected features from an experiment manager: live updates, model upload, experiment comparison etc. but ClearML also tracks uncommitted changes and installed packages for example. Thanks to that ClearML Tasks (which is what we call experiments) are also reproducible on different machines! With only 1 extra line, we can schedule a YOLOv5 training task on a queue to be executed by any number of ClearML Agents (workers).\n",
        "\n",
        "You can use ClearML Data to version your dataset and then pass it to YOLOv5 simply using its unique ID. This will help you keep track of your data without adding extra hassle. Explore the [ClearML Tutorial](https://docs.ultralytics.com/yolov5/tutorials/clearml_logging_integration) for details!\n",
        "\n",
        "<a href=\"https://cutt.ly/yolov5-notebook-clearml\">\n",
        "<img alt=\"ClearML Experiment Management UI\" src=\"https://github.com/thepycoder/clearml_screenshots/raw/main/scalars.jpg\" width=\"1280\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WPvRbS5Swl6"
      },
      "source": [
        "## Local Logging\n",
        "\n",
        "Training results are automatically logged with [Tensorboard](https://www.tensorflow.org/tensorboard) and [CSV](https://github.com/ultralytics/yolov5/pull/4148) loggers to `runs/train`, with a new experiment directory created for each new training as `runs/train/exp2`, `runs/train/exp3`, etc.\n",
        "\n",
        "This directory contains train and val statistics, mosaics, labels, predictions and augmentated mosaics, as well as metrics and charts including precision-recall (PR) curves and confusion matrices.\n",
        "\n",
        "<img alt=\"Local logging results\" src=\"https://user-images.githubusercontent.com/26833433/183222430-e1abd1b7-782c-4cde-b04d-ad52926bf818.jpg\" width=\"1280\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zelyeqbyt3GD"
      },
      "source": [
        "# Environments\n",
        "\n",
        "YOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n",
        "\n",
        "- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n",
        "- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n",
        "- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n",
        "- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Qu7Iesl0p54"
      },
      "source": [
        "# Status\n",
        "\n",
        "![YOLOv5 CI](https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg)\n",
        "\n",
        "If this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 training ([train.py](https://github.com/ultralytics/yolov5/blob/master/train.py)), testing ([val.py](https://github.com/ultralytics/yolov5/blob/master/val.py)), inference ([detect.py](https://github.com/ultralytics/yolov5/blob/master/detect.py)) and export ([export.py](https://github.com/ultralytics/yolov5/blob/master/export.py)) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEijrePND_2I"
      },
      "source": [
        "# Appendix\n",
        "\n",
        "Additional content below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMusP4OAxFu6"
      },
      "outputs": [],
      "source": [
        "# YOLOv5 PyTorch HUB Inference (DetectionModels only)\n",
        "import torch\n",
        "\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', force_reload=True, trust_repo=True)  # or yolov5n - yolov5x6 or custom\n",
        "im = 'https://ultralytics.com/images/zidane.jpg'  # file, Path, PIL.Image, OpenCV, nparray, list\n",
        "results = model(im)  # inference\n",
        "results.print()  # or .show(), .save(), .crop(), .pandas(), etc."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "YOLOv5 Tutorial",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
